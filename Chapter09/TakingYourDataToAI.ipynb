{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sWpzMt_Y35o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the text file\n",
        "path = '/content/t8.shakespeare.output.txt'\n",
        "text = open(path).read().lower()\n",
        "print('length of the corpus is: :', len(text))\n",
        "\n",
        "word_text = []\n",
        "word_count = []\n",
        "\n",
        "with open(path,'r') as file:\n",
        "\n",
        "  # reading each line\n",
        "  for line in file:\n",
        "\n",
        "    # reading each word\n",
        "    for word in line.split():\n",
        "\n",
        "      # storing the words\n",
        "      if word.isnumeric():\n",
        "        word_count.append(word)\n",
        "\n",
        "      else:\n",
        "        word = word.replace(\":\",\"\")\n",
        "        if word not in stopwords:\n",
        "          word_text.append(word)\n",
        "        else:\n",
        "          word_count.pop()\n",
        "\n",
        "print((word_text))\n",
        "print(len(word_count))\n",
        "\n",
        "word_count = [int(x) for x in word_count]"
      ],
      "metadata": {
        "id": "cNZAzvkkY7Gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df['word_text'] = word_text\n",
        "df['word_count'] = word_count\n",
        "print(len(df))\n",
        "\n",
        "df = df.sort_values(by=\"word_count\", ascending=False, kind=\"mergesort\" )\n",
        "df = df.head(500)\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "Eyy2B5MJY7Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wlist = []\n",
        "word_list = df[\"word_text\"].values.tolist()\n",
        "wlist.append(word_list)\n",
        "\n",
        "# Load pre-trained word embeddings model\n",
        "model = Word2Vec(sentences=wlist, vector_size=1, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "\n",
        "embedding_model = Word2Vec.load(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "LO8KD4-OY7Mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = [embedding_model.wv[word] for word in word_list]\n",
        "similarity_scores = []\n",
        "for i, word_vector in enumerate(word_vectors):\n",
        "  wv_list = []\n",
        "  wv_list.append(word_vector)\n",
        "  other_vectors = word_vectors[:i] + word_vectors[i+1:]\n",
        "  for other_vector in other_vectors:\n",
        "    ov_list = []\n",
        "    ov_list.append(other_vector)\n",
        "    #print(cosine_similarity(wv_list, ov_list))\n",
        "    similarity_scores = cosine_similarity(wv_list, ov_list).flatten()\n",
        "    context_similarity = np.mean([similarity_scores])\n",
        "  similarity_scores = np.append(similarity_scores, context_similarity)\n"
      ],
      "metadata": {
        "id": "cXO8lj1LY7PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 10\n",
        "\n",
        "# Get top-k words based on similarity scores\n",
        "context_indices = np.argsort(similarity_scores)[-top_k:]\n",
        "context = [word_list[index] for index in context_indices]\n",
        "\n",
        "# Display the extracted context\n",
        "print(\"Extracted context from the given word cloud:\")\n",
        "print(context)"
      ],
      "metadata": {
        "id": "3nE6Dsv7ZMi8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}